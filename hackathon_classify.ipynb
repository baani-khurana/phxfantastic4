{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import keras\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('severeinjury.csv')\n",
    "df['Part of Body Title'] = [e.split()[0] for e in df['Part of Body Title']]\n",
    "df['Part of Body Title'] = [e.replace(',','') for e in df['Part of Body Title']]\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dirty = df['Final Narrative']\n",
    "y_dirty = df['Part of Body Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.array(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentences(sentences):\n",
    "    translator = str.maketrans('', '', string.punctuation + string.digits)\n",
    "    print('Starting translations...')\n",
    "    sentences = [s.translate(translator) for s in sentences]\n",
    "    stopset = set(nltk.corpus.stopwords.words('english'))\n",
    "    print('Lowercasing...')\n",
    "    tokens = [nltk.wordpunct_tokenize(s.lower()) for s in sentences]\n",
    "    print('Splitting...')\n",
    "    tokens = [np.array(t)[np.invert(np.isin(t, list(stopset)))] for t in tokens]\n",
    "    return np.array(tokens)\n",
    "\n",
    "tokens = clean_sentences(X_dirty)\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=1000, oov_token=1)\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "    \n",
    "X_all = []\n",
    "\n",
    "count = 0\n",
    "print('Fitting....')\n",
    "for line in tokens:\n",
    "    tokenizer.fit_on_texts(line)\n",
    "    count += 1\n",
    "    if count % 100 == 0 or count == len(X_dirty):\n",
    "        if count != 0:\n",
    "            sys.stdout.write('\\r')\n",
    "        bars = int(count/len(X_dirty)*20)\n",
    "        sys.stdout.write('[{0:20}] {1:4.1%}'.format('='*bars, count/len(X_dirty)))\n",
    "print('\\nDone fitting!')\n",
    "print('Sequencing....')\n",
    "count = 0\n",
    "for line in tokens:\n",
    "    add = [list(np.array(tokenizer.texts_to_sequences(line)).flatten())]\n",
    "    X_all += add\n",
    "    count += 1\n",
    "    if count % 100 == 0 or count == len(X_dirty):\n",
    "        if count != 0:\n",
    "            sys.stdout.write('\\r')\n",
    "        bars = int(count/len(X_dirty)*20)\n",
    "        sys.stdout.write('[{0:20}] {1:4.1%}'.format('='*bars, count/len(X_dirty)))\n",
    "print('\\nFinished Tokening!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Vocab.dat', 'w') as vfile:\n",
    "    vfile.write('{} {}'.format(0, 'x'))\n",
    "    for key, value in sorted(tokenizer.word_index.items(),\n",
    "                                 key=lambda kv: (kv[1], kv[0])):\n",
    "        vfile.write('\\n{} {}'.format(value, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_dict():\n",
    "    vocab_dict = {}\n",
    "    with open('Vocab.dat') as vocab_file:\n",
    "        for line in vocab_file:\n",
    "            (val, key) = line.split()\n",
    "            val = int(val)\n",
    "            if val == 0:\n",
    "                key = ''\n",
    "            vocab_dict[key] = val\n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence_from_dict(sentence, vocab_dict):\n",
    "    tokenized = []\n",
    "    for word in sentence.split():\n",
    "        if word in vocab_dict:\n",
    "            tokenized += [vocab_dict[word]]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_sentence(model, sentence):\n",
    "    vocab_dict = get_vocab_dict()\n",
    "    tokenized = tokenize_sentence_from_dict(sentence, vocab_dict)\n",
    "    return model.predict(np.array([tokenized]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(X_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_all = np.load('X_all.npy')\n",
    "#y_all = np.load('y_all.npy')\n",
    "\n",
    "X_all = keras.preprocessing.sequence.pad_sequences(X_all, 40)\n",
    "y_all = np.array(pd.get_dummies(y_dirty))\n",
    "\n",
    "np.save('X_all.npy', X_all)\n",
    "np.save('y_all.npy', y_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(X_all)))\n",
    "np.random.shuffle(indices)\n",
    "num_train = int(.75*len(X_all))\n",
    "\n",
    "X_train = X_all[indices[:num_train]]\n",
    "X_test = X_all[indices[num_train:]]\n",
    "y_train = y_all[indices[:num_train]]\n",
    "y_test = y_all[indices[num_train:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(keras.layers.Embedding(12000, 50, mask_zero=True))\n",
    "    model.add(keras.layers.LSTM(64))\n",
    "    model.add(keras.layers.Dense(256, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(.5))\n",
    "    model.add(keras.layers.Dense(61, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )   \n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"body_parts2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6405\n",
    "test_sentence = df['Final Narrative'][6405]\n",
    "#test_sentence = 'The employee fell off of the ladder and hit his head.'\n",
    "print(\"Sentence: \\n\\n{}\".format(test_sentence))\n",
    "model = keras.models.load_model(\"body_parts.h5\")\n",
    "pred = predict_from_sentence(model, test_sentence)\n",
    "idx = np.argmax(pred)\n",
    "print(\"\\nPrediction: \\n{}\".format(pd.get_dummies(y_dirty).columns[idx]))\n",
    "print(\"\\nActual: \\n{}\".format(df['Part of Body Title'][6405]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pd.get_dummies(y_dirty).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
