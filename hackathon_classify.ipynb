{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import keras\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>UPA</th>\n",
       "      <th>EventDate</th>\n",
       "      <th>Employer</th>\n",
       "      <th>Address1</th>\n",
       "      <th>Address2</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>...</th>\n",
       "      <th>Nature</th>\n",
       "      <th>NatureTitle</th>\n",
       "      <th>Part of Body</th>\n",
       "      <th>Part of Body Title</th>\n",
       "      <th>Event</th>\n",
       "      <th>EventTitle</th>\n",
       "      <th>Source</th>\n",
       "      <th>SourceTitle</th>\n",
       "      <th>Secondary Source</th>\n",
       "      <th>Secondary Source Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015010015</td>\n",
       "      <td>931176</td>\n",
       "      <td>1/1/2015</td>\n",
       "      <td>FCI Otisville Federal Correctional Institution</td>\n",
       "      <td>Two Mile Drive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OTISVILLE</td>\n",
       "      <td>NEW YORK</td>\n",
       "      <td>10963.0</td>\n",
       "      <td>41.46</td>\n",
       "      <td>...</td>\n",
       "      <td>111</td>\n",
       "      <td>Fractures</td>\n",
       "      <td>513</td>\n",
       "      <td>Lower</td>\n",
       "      <td>1214</td>\n",
       "      <td>Injured by physical contact with person while ...</td>\n",
       "      <td>5721</td>\n",
       "      <td>Co-worker</td>\n",
       "      <td>5772.0</td>\n",
       "      <td>Inmate or detainee in custody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015010016</td>\n",
       "      <td>930267</td>\n",
       "      <td>1/1/2015</td>\n",
       "      <td>Kalahari Manufacturing LLC</td>\n",
       "      <td>171 Progress Drive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LAKE DELTON</td>\n",
       "      <td>WISCONSIN</td>\n",
       "      <td>53940.0</td>\n",
       "      <td>43.59</td>\n",
       "      <td>...</td>\n",
       "      <td>1522</td>\n",
       "      <td>Second</td>\n",
       "      <td>519</td>\n",
       "      <td>Leg(s)</td>\n",
       "      <td>317</td>\n",
       "      <td>Ignition of vapors, gases, or liquids</td>\n",
       "      <td>7261</td>\n",
       "      <td>Welding, cutting, and blow torches</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015010018</td>\n",
       "      <td>929823</td>\n",
       "      <td>1/1/2015</td>\n",
       "      <td>Schneider National Bulk Carrier</td>\n",
       "      <td>420 CORAOPOLIS ROAD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CORAOPOLIS</td>\n",
       "      <td>PENNSYLVANIA</td>\n",
       "      <td>15108.0</td>\n",
       "      <td>40.49</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>Traumatic</td>\n",
       "      <td>9999</td>\n",
       "      <td>Nonclassifiable</td>\n",
       "      <td>4331</td>\n",
       "      <td>Other fall to lower level less than 6 feet</td>\n",
       "      <td>8421</td>\n",
       "      <td>Semi, tractor-trailer, tanker truck</td>\n",
       "      <td>741.0</td>\n",
       "      <td>Ladders-fixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015010019</td>\n",
       "      <td>929711</td>\n",
       "      <td>1/1/2015</td>\n",
       "      <td>PEPSI BOTTLING GROUP INC.</td>\n",
       "      <td>4541 HOUSTON AVE.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MACON</td>\n",
       "      <td>GEORGIA</td>\n",
       "      <td>31206.0</td>\n",
       "      <td>32.77</td>\n",
       "      <td>...</td>\n",
       "      <td>1972</td>\n",
       "      <td>Soreness</td>\n",
       "      <td>510</td>\n",
       "      <td>Leg(s)</td>\n",
       "      <td>640</td>\n",
       "      <td>Caught in or compressed by equipment or object...</td>\n",
       "      <td>8623</td>\n",
       "      <td>Pallet jack-powered</td>\n",
       "      <td>8420.0</td>\n",
       "      <td>Truck-motorized freight hauling and utility, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015010020</td>\n",
       "      <td>929642</td>\n",
       "      <td>1/1/2015</td>\n",
       "      <td>North American Pipe Corporation</td>\n",
       "      <td>210 South Arch Street</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JANESVILLE</td>\n",
       "      <td>WISCONSIN</td>\n",
       "      <td>53545.0</td>\n",
       "      <td>42.67</td>\n",
       "      <td>...</td>\n",
       "      <td>111</td>\n",
       "      <td>Fractures</td>\n",
       "      <td>4429</td>\n",
       "      <td>Finger(s)</td>\n",
       "      <td>6411</td>\n",
       "      <td>Caught in running equipment or machinery durin...</td>\n",
       "      <td>350</td>\n",
       "      <td>Metal, woodworking, and special material machi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID     UPA EventDate  \\\n",
       "0  2015010015  931176  1/1/2015   \n",
       "1  2015010016  930267  1/1/2015   \n",
       "2  2015010018  929823  1/1/2015   \n",
       "3  2015010019  929711  1/1/2015   \n",
       "4  2015010020  929642  1/1/2015   \n",
       "\n",
       "                                         Employer               Address1  \\\n",
       "0  FCI Otisville Federal Correctional Institution         Two Mile Drive   \n",
       "1                      Kalahari Manufacturing LLC     171 Progress Drive   \n",
       "2                 Schneider National Bulk Carrier    420 CORAOPOLIS ROAD   \n",
       "3                       PEPSI BOTTLING GROUP INC.      4541 HOUSTON AVE.   \n",
       "4                 North American Pipe Corporation  210 South Arch Street   \n",
       "\n",
       "  Address2         City         State      Zip  Latitude  \\\n",
       "0      NaN    OTISVILLE      NEW YORK  10963.0     41.46   \n",
       "1      NaN  LAKE DELTON     WISCONSIN  53940.0     43.59   \n",
       "2      NaN   CORAOPOLIS  PENNSYLVANIA  15108.0     40.49   \n",
       "3      NaN        MACON       GEORGIA  31206.0     32.77   \n",
       "4      NaN   JANESVILLE     WISCONSIN  53545.0     42.67   \n",
       "\n",
       "                         ...                          Nature NatureTitle  \\\n",
       "0                        ...                             111   Fractures   \n",
       "1                        ...                            1522      Second   \n",
       "2                        ...                              10   Traumatic   \n",
       "3                        ...                            1972    Soreness   \n",
       "4                        ...                             111   Fractures   \n",
       "\n",
       "   Part of Body  Part of Body Title  Event  \\\n",
       "0           513               Lower   1214   \n",
       "1           519              Leg(s)    317   \n",
       "2          9999     Nonclassifiable   4331   \n",
       "3           510              Leg(s)    640   \n",
       "4          4429           Finger(s)   6411   \n",
       "\n",
       "                                          EventTitle  Source  \\\n",
       "0  Injured by physical contact with person while ...    5721   \n",
       "1              Ignition of vapors, gases, or liquids    7261   \n",
       "2         Other fall to lower level less than 6 feet    8421   \n",
       "3  Caught in or compressed by equipment or object...    8623   \n",
       "4  Caught in running equipment or machinery durin...     350   \n",
       "\n",
       "                                         SourceTitle  Secondary Source  \\\n",
       "0                                          Co-worker            5772.0   \n",
       "1                 Welding, cutting, and blow torches               NaN   \n",
       "2                Semi, tractor-trailer, tanker truck             741.0   \n",
       "3                                Pallet jack-powered            8420.0   \n",
       "4  Metal, woodworking, and special material machi...               NaN   \n",
       "\n",
       "                              Secondary Source Title  \n",
       "0                      Inmate or detainee in custody  \n",
       "1                                                NaN  \n",
       "2                                      Ladders-fixed  \n",
       "3  Truck-motorized freight hauling and utility, u...  \n",
       "4                                                NaN  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up dataframe\n",
    "# Basic cleaning to keep it simple\n",
    "\n",
    "df = pd.read_csv('severeinjury.csv')\n",
    "df['Part of Body Title'] = [e.split()[0] for e in df['Part of Body Title']]\n",
    "df['Part of Body Title'] = [e.replace(',','') for e in df['Part of Body Title']]\n",
    "df['NatureTitle'] = [e.split()[0] for e in df['NatureTitle']]\n",
    "df['NatureTitle'] = [e.replace(',','') for e in df['NatureTitle']]\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set names of feature/target variables\n",
    "\n",
    "X_dirty = df['Final Narrative']\n",
    "y_body_dirty = df['Part of Body Title']\n",
    "y_nature_dirty = df['NatureTitle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in a sentence and prepares it to be tokenized\n",
    "\n",
    "def clean_sentences(sentences):\n",
    "    translator = str.maketrans('', '', string.punctuation + string.digits)\n",
    "    print('Starting translations...')\n",
    "    sentences = [s.translate(translator) for s in sentences]\n",
    "    stopset = set(nltk.corpus.stopwords.words('english'))\n",
    "    print('Lowercasing...')\n",
    "    tokens = [nltk.wordpunct_tokenize(s.lower()) for s in sentences]\n",
    "    print('Splitting...')\n",
    "    tokens = [np.array(t)[np.invert(np.isin(t, list(stopset)))] for t in tokens]\n",
    "    return np.array(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting translations...\n",
      "Lowercasing...\n",
      "Splitting...\n",
      "Fitting....\n",
      "[====================] 100.0%\n",
      "Done fitting!\n",
      "Sequencing....\n",
      "[====================] 100.0%\n",
      "Finished Tokening!\n"
     ]
    }
   ],
   "source": [
    "# Clean and tokenize sentences\n",
    "\n",
    "tokens = clean_sentences(X_dirty)\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "    \n",
    "X_all = []\n",
    "\n",
    "count = 0\n",
    "print('Fitting....')\n",
    "\n",
    "# Add all words to dictionary\n",
    "for line in tokens:\n",
    "    tokenizer.fit_on_texts(line)\n",
    "    count += 1\n",
    "    if count % 100 == 0 or count == len(X_dirty):\n",
    "        if count != 0:\n",
    "            sys.stdout.write('\\r')\n",
    "        bars = int(count/len(X_dirty)*20)\n",
    "        sys.stdout.write('[{0:20}] {1:4.1%}'.format('='*bars, count/len(X_dirty)))\n",
    "print('\\nDone fitting!')\n",
    "print('Sequencing....')\n",
    "count = 0\n",
    "\n",
    "# Convert all strings to dictionary indices\n",
    "for line in tokens:\n",
    "    add = [list(np.array(tokenizer.texts_to_sequences(line)).flatten())]\n",
    "    X_all += add\n",
    "    count += 1\n",
    "    if count % 100 == 0 or count == len(X_dirty):\n",
    "        if count != 0:\n",
    "            sys.stdout.write('\\r')\n",
    "        bars = int(count/len(X_dirty)*20)\n",
    "        sys.stdout.write('[{0:20}] {1:4.1%}'.format('='*bars, count/len(X_dirty)))\n",
    "print('\\nFinished Tokening!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vocab file for future use (Prediction demos)\n",
    "\n",
    "with open('Vocab.dat', 'w') as vfile:\n",
    "    vfile.write('{} {}'.format(0, 'x'))\n",
    "    for key, value in sorted(tokenizer.word_index.items(),\n",
    "                                 key=lambda kv: (kv[1], kv[0])):\n",
    "        vfile.write('\\n{} {}'.format(value, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for prediction demo\n",
    "# Retrieve vocab dictionary from previously saved file\n",
    "\n",
    "def get_vocab_dict():\n",
    "    vocab_dict = {}\n",
    "    with open('Vocab.dat') as vocab_file:\n",
    "        for line in vocab_file:\n",
    "            (val, key) = line.split()\n",
    "            val = int(val)\n",
    "            if val == 0:\n",
    "                key = ''\n",
    "            vocab_dict[key] = val\n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to tokenize a sentence from premade dictionary\n",
    "\n",
    "def tokenize_sentence_from_dict(sentence, vocab_dict):\n",
    "    tokenized = []\n",
    "    for word in sentence.split():\n",
    "        if word in vocab_dict:\n",
    "            tokenized += [vocab_dict[word]]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a prediction easily from a sentence\n",
    "\n",
    "def predict_from_sentence(model, sentence):\n",
    "    vocab_dict = get_vocab_dict()\n",
    "    tokenized = tokenize_sentence_from_dict(sentence, vocab_dict)\n",
    "    return model.predict(np.array([tokenized]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to save time, we can load previously saved tokenized sentences\n",
    "# Saves a few minutes for when we want to debug mostly\n",
    "#X_all = np.load('X_all.npy')\n",
    "#y_all = np.load('y_all.npy')\n",
    "\n",
    "# Pad sentences to make them all uniform length\n",
    "X_all = keras.preprocessing.sequence.pad_sequences(X_all, 50)\n",
    "# Convert labels to categorical data\n",
    "y_body_all = np.array(pd.get_dummies(y_body_dirty))\n",
    "y_nature_all = np.array(pd.get_dummies(y_nature_dirty))\n",
    "\n",
    "# Save tokenizes sentences for future debugging use\n",
    "np.save('X_all.npy', X_all)\n",
    "np.save('y_body_all.npy', y_body_all)\n",
    "np.save('y_nature_all.npy', y_nature_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and test sets\n",
    "\n",
    "indices = list(range(len(X_all)))\n",
    "np.random.shuffle(indices)\n",
    "num_train = int(.75*len(X_all))\n",
    "\n",
    "X_train = X_all[indices[:num_train]]\n",
    "X_test = X_all[indices[num_train:]]\n",
    "y_body_train = y_body_all[indices[:num_train]]\n",
    "y_body_test = y_body_all[indices[num_train:]]\n",
    "y_nature_train = y_nature_all[indices[:num_train]]\n",
    "y_nature_test = y_nature_all[indices[num_train:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Neural Network Helper Method\n",
    "\n",
    "def build_model(num_output):\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(keras.layers.Embedding(12000, 64, mask_zero=True))\n",
    "    model.add(keras.layers.LSTM(64))\n",
    "    model.add(keras.layers.Dropout(.5))\n",
    "    model.add(keras.layers.Dense(256, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(.5))\n",
    "    model.add(keras.layers.Dense(num_output, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )   \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16183 samples, validate on 5395 samples\n",
      "Epoch 1/10\n",
      "16183/16183 [==============================] - 35s 2ms/step - loss: 2.3945 - acc: 0.3485 - val_loss: 1.5097 - val_acc: 0.5965\n",
      "Epoch 2/10\n",
      "16183/16183 [==============================] - 34s 2ms/step - loss: 1.3247 - acc: 0.6486 - val_loss: 1.0416 - val_acc: 0.7286\n",
      "Epoch 3/10\n",
      "16183/16183 [==============================] - 34s 2ms/step - loss: 0.9369 - acc: 0.7495 - val_loss: 0.8618 - val_acc: 0.7796\n",
      "Epoch 4/10\n",
      "16183/16183 [==============================] - 34s 2ms/step - loss: 0.7419 - acc: 0.7963 - val_loss: 0.8529 - val_acc: 0.7905\n",
      "Epoch 5/10\n",
      "16183/16183 [==============================] - 34s 2ms/step - loss: 0.6198 - acc: 0.8261 - val_loss: 0.7840 - val_acc: 0.8076\n",
      "Epoch 6/10\n",
      "16183/16183 [==============================] - 34s 2ms/step - loss: 0.5385 - acc: 0.8470 - val_loss: 0.8053 - val_acc: 0.8082\n",
      "Epoch 7/10\n",
      "16183/16183 [==============================] - 34s 2ms/step - loss: 0.4665 - acc: 0.8655 - val_loss: 0.8158 - val_acc: 0.8152\n",
      "Epoch 8/10\n",
      "16183/16183 [==============================] - 34s 2ms/step - loss: 0.4296 - acc: 0.8767 - val_loss: 0.8175 - val_acc: 0.8115\n",
      "Epoch 9/10\n",
      "16183/16183 [==============================] - 34s 2ms/step - loss: 0.3690 - acc: 0.8936 - val_loss: 0.8958 - val_acc: 0.8117\n",
      "Epoch 10/10\n",
      "16183/16183 [==============================] - 34s 2ms/step - loss: 0.3468 - acc: 0.9019 - val_loss: 0.9120 - val_acc: 0.8057\n",
      "Train on 16183 samples, validate on 5395 samples\n",
      "Epoch 1/10\n",
      "16183/16183 [==============================] - 35s 2ms/step - loss: 1.7300 - acc: 0.5485 - val_loss: 1.1350 - val_acc: 0.6906\n",
      "Epoch 2/10\n",
      "16183/16183 [==============================] - 34s 2ms/step - loss: 1.0313 - acc: 0.7067 - val_loss: 0.8862 - val_acc: 0.7464\n",
      "Epoch 3/10\n",
      "16183/16183 [==============================] - 34s 2ms/step - loss: 0.7966 - acc: 0.7720 - val_loss: 0.8078 - val_acc: 0.7918\n",
      "Epoch 4/10\n",
      "16183/16183 [==============================] - 34s 2ms/step - loss: 0.6382 - acc: 0.8225 - val_loss: 0.7369 - val_acc: 0.8174\n",
      "Epoch 5/10\n",
      "16183/16183 [==============================] - 34s 2ms/step - loss: 0.5221 - acc: 0.8582 - val_loss: 0.7382 - val_acc: 0.8252\n",
      "Epoch 6/10\n",
      "16183/16183 [==============================] - 34s 2ms/step - loss: 0.4558 - acc: 0.8755 - val_loss: 0.7495 - val_acc: 0.8289\n",
      "Epoch 7/10\n",
      "16183/16183 [==============================] - 34s 2ms/step - loss: 0.3947 - acc: 0.8906 - val_loss: 0.8101 - val_acc: 0.8295\n",
      "Epoch 8/10\n",
      "16183/16183 [==============================] - 36s 2ms/step - loss: 0.3374 - acc: 0.9045 - val_loss: 0.7858 - val_acc: 0.8367\n",
      "Epoch 9/10\n",
      "16183/16183 [==============================] - 36s 2ms/step - loss: 0.3121 - acc: 0.9111 - val_loss: 0.8033 - val_acc: 0.8397\n",
      "Epoch 10/10\n",
      "16183/16183 [==============================] - 35s 2ms/step - loss: 0.2754 - acc: 0.9209 - val_loss: 0.8396 - val_acc: 0.8452\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd0dd9b1860>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train models individually\n",
    "\n",
    "model_body = build_model(len(pd.get_dummies(y_body_dirty).columns))\n",
    "model_body.fit(X_train, y_body_train, validation_data=(X_test, y_body_test), epochs=10)\n",
    "\n",
    "model_nature = build_model(len(pd.get_dummies(y_nature_dirty).columns))\n",
    "model_nature.fit(X_train, y_nature_train, validation_data=(X_test, y_nature_test), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models just in case\n",
    "\n",
    "model_body.save(\"body_parts.h5\")\n",
    "model_nature.save(\"nature.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \n",
      "\n",
      "An employee fell approximately 8 feet and hit his head on the ground while descending an extension ladder. He fractured the orbital bone in his head.\n",
      "\n",
      "Prediction: \n",
      "\tBody:   Eye(s)\n",
      "\tNature: Fractures\n",
      "\n",
      "Actual: \n",
      "\tBody:   Eye(s)\n",
      "\tNature: Fractures\n"
     ]
    }
   ],
   "source": [
    "# Useful cell for the demo!\n",
    "# 6405 is an interesting index to look at\n",
    "\n",
    "test_idx = 6969\n",
    "test_sentence = df['Final Narrative'][test_idx]\n",
    "\n",
    "# Can take audience input as well!\n",
    "# test_sentence = 'The employee got his foot stuck in a machine. He lost a toe.'\n",
    "\n",
    "print(\"Sentence: \\n\\n{}\".format(test_sentence))\n",
    "model_body = keras.models.load_model(\"body_parts2.h5\")\n",
    "model_nature = keras.models.load_model(\"nature.h5\")\n",
    "pred_body = predict_from_sentence(model_body, test_sentence)\n",
    "pred_nature = predict_from_sentence(model_nature, test_sentence)\n",
    "idx_body = np.argmax(pred_body)\n",
    "idx_nature = np.argmax(pred_nature)\n",
    "print(\"\\nPrediction: \\n\\tBody:   {}\\n\\tNature: {}\".format(pd.get_dummies(y_body_dirty).columns[idx_body], pd.get_dummies(y_nature_dirty).columns[idx_nature]))\n",
    "print(\"\\nActual: \\n\\tBody:   {}\\n\\tNature: {}\".format(df['Part of Body Title'][test_idx], df['NatureTitle'][test_idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
